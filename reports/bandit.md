# 🧪 Experiment Report: Greedy Bandit (Baseline)

## 1. 실험 개요 및 선정 이유

Bandit 알고리즘은 상태(State) 전이를 고려하지 않고, 현재 시점에서 가장 이득이 되는 행동(Greedy)을 취하는 단순한 방식입니다. 본 프로젝트에서는 강화학습(RL)이 정말로 무작위 탐색보다 효과적인지 검증하기 위한 **비교 기준점(Baseline)**으로 선정하였습니다.

- **알고리즘**: $\epsilon$-Greedy Bandit
- **전략**:
  - **Exploit (70%)**: 현재 인력 부족(Shortage)이 가장 심각한 날짜를 찾아 수정 시도.
  - **Explore (30%)**: 무작위 위치를 수정하여 Local Optima 탈출 시도.

## 2. 실험 환경 (Experimental Setup)

| 항목            | 값  | 비고                  |
| :-------------- | :-- | :-------------------- |
| **Scenario ID** | 1   | 30 Nurses, 31 Days    |
| **Episodes**    | 500 |                       |
| **Max Steps**   | 200 | 에피소드 당 수정 횟수 |
| **Random Seed** | 42  |                       |
| **Epsilon**     | 0.3 | 높은 탐험률 유지      |

## 3. 실험 결과 (Results)

### 3.1 학습 곡선 (Learning Curve)

- **Total Reward**: 에피소드마다 -100 ~ +100 사이를 진동하며, 뚜렷한 상승 추세가 보이지 않습니다.
- **Final Score**: 초기 -3000점대에서 시작하여 -2600점대까지 개선되나, 그 이상 좋아지지 않습니다.

### 3.2 제약 조건 만족도 (Constraints)

- **Hard Violations**: 평균 200개 -> 170개 수준으로 소폭 감소.
- **Coverage Shortage**: 인력 부족은 비교적 잘 해결하지만(부족한 곳을 타겟팅하므로), 연속 근무 제한 같은 복잡한 패턴은 해결하지 못합니다.

## 4. 결론 및 고찰

Bandit은 **"급한 불(인력 펑크)"을 끄는 데는 효과적**이지만, **"전체적인 조화(연속 근무, 패턴)"를 맞추는 데는 한계**가 명확합니다. 상태(State)를 기억하지 못하므로 매번 새로운 실수를 반복하는 경향이 있습니다. 이를 해결하기 위해 상태 전이를 학습하는 DQN, PPO 등의 RL 알고리즘이 필요함을 확인했습니다.
